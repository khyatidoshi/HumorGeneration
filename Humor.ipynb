{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl+58Pr0c1l3j4R5kaSD1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khyatidoshi/HumorGeneration/blob/main/Humor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B1evDTOyBfus"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchtext as tt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joke_dataset = pd.read_csv('new_joke_data.csv')\n",
        "joke_dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "K6TufqGrBooX",
        "outputId": "125b3ba8-2913-4d6d-89eb-4049c12c63df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID                                               Joke\n",
              "0   0  [me narrating a documentary about narrators] \"...\n",
              "1   1  Telling my daughter garlic is good for you. Go...\n",
              "2   2  I've been going through a really rough period ...\n",
              "3   3  If I could have dinner with anyone, dead or al...\n",
              "4   4     Two guys walk into a bar. The third guy ducks."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-049e8bb2-34dc-4dc6-84c9-28eeedd9417b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Joke</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[me narrating a documentary about narrators] \"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Telling my daughter garlic is good for you. Go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I've been going through a really rough period ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>If I could have dinner with anyone, dead or al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Two guys walk into a bar. The third guy ducks.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-049e8bb2-34dc-4dc6-84c9-28eeedd9417b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-049e8bb2-34dc-4dc6-84c9-28eeedd9417b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-049e8bb2-34dc-4dc6-84c9-28eeedd9417b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dd507041-0c53-4f4e-b1f6-845ebc32a8fd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd507041-0c53-4f4e-b1f6-845ebc32a8fd')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dd507041-0c53-4f4e-b1f6-845ebc32a8fd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "  def __init__(self, dataset, sequence_length=64):\n",
        "    self.sequence_length = sequence_length\n",
        "    self.words = self.load_words(dataset)\n",
        "    self.uniq_words = self.get_uniq_words()\n",
        "    self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "    self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "    self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "  def load_words(self, dataset):\n",
        "    text = \" \".join(dataset['Joke'])\n",
        "    return text.split(' ')\n",
        "\n",
        "  def get_uniq_words(self):\n",
        "    word_counts = Counter(self.words)\n",
        "    return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.words_indexes) - self.sequence_length\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return (\n",
        "      torch.tensor(self.words_indexes[index:index + self.sequence_length]),\n",
        "      torch.tensor(self.words_indexes[index + 1:index + self.sequence_length + 1]),\n",
        "    )"
      ],
      "metadata": {
        "id": "2MBOmzQ-nagv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, df, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(Model, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.df = df\n",
        "        self.rnn = getattr(nn, 'LSTM')(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        # Note that Input comes in as size (50 x 64)\n",
        "        # This is 50 samples, taking 64 tokens of each joke (indices for each token)\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        # We embed the indices as vectors - each index is going be represented by a vector of length 64\n",
        "        # So that emb now as size (50 x 64 x 64) - each of the 64 tokens is now represented by a vector of length 64\n",
        "        # print(\" here  maybe\")\n",
        "        output = torch.zeros( ( emb.size(0), self.ninp ) )\n",
        "        # Rather than append outputs to a list, I'm constructing the full output tensor in advance\n",
        "        # And then I'll just fill it in\n",
        "\n",
        "\n",
        "        for i in range(emb.size(0)):\n",
        "            # print(\" here \")\n",
        "            rnn_output, hidden = self.rnn(emb[i].view(1, self.ninp, -1), hidden)\n",
        "            # So note that emb[i] is of size (64 (tokens) x 64 (vector for each token))\n",
        "            # emb[i].view(1, 64, -1) resizes it to [1 (sample) x 64 (tokens) x 64 (vector for each token)]\n",
        "            # PREVIOUSLY: You had emb[i].view(1, 1, -1), which turned it into a tensor [ 1 x (64*64) ]\n",
        "            # But this new version keeps each token separated rather than turning it into one really long vector\n",
        "\n",
        "\n",
        "            # At this point, rnn_output has shape [1, 64, 1024], a sequence of 64 large calculated vectors\n",
        "            # self.drop(rnn_output) - we drop some components but the size doesn't change\n",
        "            rnn_output = self.drop(rnn_output)\n",
        "            char_scores = self.decoder(rnn_output)\n",
        "            # print(char_scores.size())\n",
        "            # char_scores has size [1, 64, 163945]\n",
        "            # For each of the 64 tokens, we have 163945 different values, amounting to a 'score' for each\n",
        "            # Of the available tokens - this score is a simple linear function of the rnn_output terms\n",
        "\n",
        "            # We want to turn each of these 'scores' into a probability for each token\n",
        "\n",
        "            char_prob = F.log_softmax(char_scores, dim=1)\n",
        "            actual_probs = torch.exp(char_prob)\n",
        "\n",
        "            # actual_probs is of size [1, 64, 163945], for each of the 64 terms, we have 163945 probabilities\n",
        "            # one probability for each possible token\n",
        "\n",
        "\n",
        "            # Sampling the next character based on the probability distribution\n",
        "            next_char = torch.multinomial(actual_probs[0], 1)\n",
        "            # This is going to pick an integer (one of the token indices) based on the probabilities\n",
        "            # So next_char is going to have shape [64, 1] - one integer for each of the 64 terms\n",
        "            output[i] = next_char.view(-1)\n",
        "            # Saves the selected integer into the output\n",
        "\n",
        "        # The final shape of output is [50, 64] - for each of the 50 samples, we've generated a prediction of\n",
        "        # 64 integer indices for each token\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "    def init_state(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n"
      ],
      "metadata": {
        "id": "ab_uMROmVLfn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = Dataset(joke_dataset)\n",
        "ntoken = len(df.uniq_words)\n",
        "\n",
        "model = Model(df,ntoken, ninp = 64 , nhid=256, nlayers=3, dropout=0.5, tie_weights=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "G0qCINjqn1Jv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataset, bs, num_epochs, learning_rate):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
        "    print(train_loader.batch_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        print(\" EPOC \",epoch)\n",
        "        for inputs, targets in train_loader:\n",
        "            # print(\" HERE \",inputs.size(),targets.size())\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Initialize hidden state for each batch\n",
        "            hidden = model.init_state(bs)\n",
        "\n",
        "            outputs, _ = model(inputs, hidden)  # The LSTM hidden state is now handled internally\n",
        "            # print(\" HERE \",outputs.size(),targets.size())\n",
        "\n",
        "            # Cast to float\n",
        "            float_tensor = targets.float()\n",
        "            targets = torch.tensor(float_tensor, dtype=torch.float, requires_grad=True)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            print(\"epoc \", epoch,\"Loss: \",total_loss)\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    print(\"Training finished.\")\n"
      ],
      "metadata": {
        "id": "1rUBP4L78DX_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_model(model, df, batch_size, num_epochs, learning_rate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaGwelxEAkur",
        "outputId": "9988e8f4-d6c8-402e-c1bf-6356fc1cf08a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            " EPOC  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-1852e515eeea>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(float_tensor, dtype=torch.float, requires_grad=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoc  0 Loss:  68116537344.0\n",
            "epoc  0 Loss:  147088015360.0\n",
            "epoc  0 Loss:  222600175616.0\n",
            "epoc  0 Loss:  286442446848.0\n",
            "epoc  0 Loss:  353752530944.0\n",
            "epoc  0 Loss:  426754129920.0\n",
            "epoc  0 Loss:  506541383680.0\n",
            "epoc  0 Loss:  576851550208.0\n",
            "epoc  0 Loss:  649847676928.0\n",
            "epoc  0 Loss:  721930108928.0\n",
            "epoc  0 Loss:  789547495424.0\n",
            "epoc  0 Loss:  865959014400.0\n",
            "epoc  0 Loss:  946460078080.0\n",
            "epoc  0 Loss:  1020308746240.0\n",
            "epoc  0 Loss:  1098228649984.0\n",
            "epoc  0 Loss:  1170977607680.0\n",
            "epoc  0 Loss:  1243142787072.0\n",
            "epoc  0 Loss:  1313614696448.0\n",
            "epoc  0 Loss:  1388045012992.0\n",
            "epoc  0 Loss:  1463498174464.0\n",
            "epoc  0 Loss:  1536194850816.0\n",
            "epoc  0 Loss:  1612779614208.0\n",
            "epoc  0 Loss:  1684526452736.0\n",
            "epoc  0 Loss:  1751269380096.0\n",
            "epoc  0 Loss:  1826859913216.0\n",
            "epoc  0 Loss:  1908610338816.0\n",
            "epoc  0 Loss:  1987209531392.0\n",
            "epoc  0 Loss:  2064027725824.0\n",
            "epoc  0 Loss:  2143568793600.0\n",
            "epoc  0 Loss:  2213032267776.0\n",
            "epoc  0 Loss:  2288873959424.0\n",
            "epoc  0 Loss:  2357857046528.0\n",
            "epoc  0 Loss:  2443134595072.0\n",
            "epoc  0 Loss:  2518056439808.0\n",
            "epoc  0 Loss:  2578615971840.0\n",
            "epoc  0 Loss:  2653665710080.0\n",
            "epoc  0 Loss:  2730920980480.0\n",
            "epoc  0 Loss:  2805735108608.0\n",
            "epoc  0 Loss:  2871958974464.0\n"
          ]
        }
      ]
    }
  ]
}